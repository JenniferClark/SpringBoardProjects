{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a8043461-ac29-4e8b-b5a8-1f995a725a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# complete BLS toolkit\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "591f575b-d282-435d-a98f-bd01ddafeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Paths\n",
    "# ---------------------------\n",
    "DATA_DIR = \"data/processed_data\"   \n",
    "OUT_DIR  = \"data/processed_data\" \n",
    "RAW_DIR = \"data/raw\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c05db254-f01c-48f7-b8f5-143d18d47787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 0) Generic helpers\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def save_csv_file(df: pd.DataFrame, folder: str, filename: str) -> str:\n",
    "    \"\"\"Save DataFrame to CSV and return full path.\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    path = os.path.join(folder, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path} | shape={df.shape}\")\n",
    "    return path\n",
    "\n",
    "def _load_csv(name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV from DATA_DIR and normalize column names.\"\"\"\n",
    "    path = os.path.join(DATA_DIR, name)\n",
    "    df = pd.read_csv(path)\n",
    "    return _norm_colnames(df)\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize column names for BLS sheets.\"\"\"\n",
    "    cols = pd.Index(df.columns).map(str)\n",
    "    cols = (\n",
    "        cols.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.replace(r\"\\[.*?\\]\", \"\", regex=True)\n",
    "            .str.replace(\",\", \"\", regex=True)\n",
    "            .str.replace(\"—\", \"-\", regex=False)\n",
    "            .str.replace(\"–\", \"-\", regex=False)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .str.replace(\" \", \"_\", regex=False)\n",
    "    )\n",
    "    out = df.copy()\n",
    "    out.columns = cols\n",
    "    return out\n",
    "\n",
    "def _norm_colnames(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize column names for WEF/Kaggle-like CSVs.\"\"\"\n",
    "    cols = (pd.Index(df.columns).map(str)\n",
    "            .str.normalize(\"NFKC\")\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .str.replace(\" \", \"_\", regex=False)\n",
    "            .str.replace(\"%\", \"pct\", regex=False)\n",
    "            .str.replace(r\"[()]\", \"\", regex=True))\n",
    "    out = df.copy()\n",
    "    out.columns = cols\n",
    "    return out\n",
    "\n",
    "\n",
    "def normalize_dash_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replace lone dash or em-dash placeholders with NaN in object columns ONLY.\"\"\"\n",
    "    obj_cols = df.select_dtypes(include=\"object\").columns\n",
    "    if not len(obj_cols):\n",
    "        return df\n",
    "    pat = r\"\\s*[—-]\\s*\"\n",
    "    out = df.copy()\n",
    "    for c in obj_cols:\n",
    "        ser = out[c]\n",
    "        mask = ser.astype(str).str.fullmatch(pat, na=False)\n",
    "        out.loc[mask, c] = np.nan\n",
    "    return out\n",
    "\n",
    "# -------------- used for Kaggle dataset --------------\n",
    "def safe_to_datetime(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "\n",
    "def md5_hash_text(x: str) -> str:\n",
    "    return hashlib.md5((x or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def normalize_title(title: Optional[str]) -> str:\n",
    "    t = _norm_text(title)\n",
    "    t = t.replace(\"sr.\", \"senior\").replace(\"jr.\", \"junior\")\n",
    "    t = t.replace(\"&\", \" and \")\n",
    "    t = re.sub(r\"[^a-z0-9 +/#\\-]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "    \n",
    "# -------------- end used for Kaggle dataset --------------\n",
    "\n",
    "def _norm_text(s: Optional[str]) -> str:\n",
    "    \"\"\"Unicode-safe, case-insensitive, whitespace-collapsing normalizer.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def coerce_numeric_if_exists(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Convert listed columns to numeric if present; leave others untouched.\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def _pct_bounds(df: pd.DataFrame, cols: List[str], name: str):\n",
    "    \"\"\"QC: warn if percentage columns are outside [0,100].\"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            bad = df[c].notna() & ~df[c].between(0, 100)\n",
    "            if bad.any():\n",
    "                print(f\"[QC WARN] {name}.{c} has values outside [0,100]:\")\n",
    "                print(df.loc[bad, [c]].head())\n",
    "\n",
    "def drop_footer_rows(df: pd.DataFrame, title_col: str, max_tail_check: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"Remove trailing BLS footers like 'Footnotes:', '[1] ...', 'Source: ...'.\"\"\"\n",
    "    if title_col not in df.columns:\n",
    "        return df\n",
    "    tail = df.tail(max_tail_check).copy()\n",
    "    mask = (\n",
    "        tail[title_col].astype(str).str.strip().str.lower()\n",
    "        .str.startswith((\"footnotes\", \"[1]\", \"source\"))\n",
    "    )\n",
    "    return df.drop(index=tail.index[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6ebc6f41-5bf8-41f3-85f2-107371ccadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 1) Tech classifier (SOC-gated)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Always-keep titles (substring, lowercase)\n",
    "MUST_HAVE_TITLES = [\n",
    "    \"software developers\",\n",
    "    \"data scientists\",\n",
    "    \"information security analysts\",\n",
    "    \"computer systems analysts\",\n",
    "    \"database administrators\",\n",
    "    \"database architects\",\n",
    "    \"computer network architects\",\n",
    "    \"computer user support specialists\",\n",
    "    \"web developers\",\n",
    "    \"web and digital interface designers\",\n",
    "    \"software quality assurance analysts and testers\",\n",
    "    \"computer programmers\",\n",
    "    \"computer and information systems managers\"\n",
    "]\n",
    "\n",
    "# Editable include/exclude lists\n",
    "INCLUDE_TITLES = [\n",
    "    \"computer and information systems managers\",  # 11-3021\n",
    "    \"operations research analysts\",               # 15-2031\n",
    "    \"statisticians\", \n",
    "]\n",
    "EXCLUDE_TITLES = [\n",
    "    \"security guard\",\"guards\",\"cashier\",\"teller\",\"bookkeeping\",\n",
    "    \"postal\",\"receptionist\",\"customer service\",\"building caretaker\",\n",
    "    \"data entry keyer\",\"data entry keyers\",\n",
    "    \"telecommunications equipment installer\",\"telecommunications equipment installers\",\n",
    "    \"audiovisual equipment installer\",\"audiovisual equipment installers\",\n",
    "    \"broadcast technician\",\"computer operator\",\n",
    "]\n",
    "\n",
    "# SOC rules\n",
    "SOC_ALWAYS   = {\"15\"}        # Computer & Mathematical (default keep)\n",
    "SOC_ENGINEER = {\"17\"}        # Engineers → allow with computing/electrical/electronics/robotics wording\n",
    "SOC_OPTIONAL = {\"11\",\"13\"}   # Mgmt/Business → allow with strong tech evidence\n",
    "\n",
    "# drop specific SOC-15 roles (lowercase substrings) - for future use if needed\n",
    "SOC15_EXCLUDE = set([\n",
    "    # e.g., \"actuaries\",\"mathematicians\",\"mathematical science occupations, all other\"\n",
    "])\n",
    "\n",
    "# ---------- Normalizers ----------\n",
    "def _norm_code(code: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Normalize SOC code dashes to ASCII '-' and strip.\"\"\"\n",
    "    if not isinstance(code, str):\n",
    "        return None\n",
    "    c = unicodedata.normalize(\"NFKC\", code).strip()\n",
    "    c = c.replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
    "    return c if \"-\" in c else None\n",
    "\n",
    "# Pre-normalize include/exclude lists for robust matching\n",
    "NORM_INCLUDE = [_norm_text(x) for x in (INCLUDE_TITLES + MUST_HAVE_TITLES)]\n",
    "NORM_EXCLUDE = [_norm_text(x) for x in EXCLUDE_TITLES]\n",
    "NORM_SOC15_EXCLUDE = [_norm_text(x) for x in SOC15_EXCLUDE]\n",
    "\n",
    "# Keyword patterns\n",
    "STRONG_KEYWORDS = [\n",
    "    r\"\\bsoftware\\b\", r\"\\bdeveloper\\b\", r\"\\bengineer\\b\", r\"\\bdevops\\b\",\n",
    "    r\"\\bml\\b\", r\"\\bmachine learning\\b\", r\"\\bai\\b\", r\"\\bcloud\\b\",\n",
    "    r\"\\bcyber(?:security)?\\b\", r\"\\binformation security\\b\",\n",
    "    r\"\\bdata\\b\", r\"\\banalytics?\\b\", r\"\\bdatabase\\b\",\n",
    "    r\"\\b(?:sre|site reliability)\\b\", r\"\\bnetwork(s)?\\b\",\n",
    "    r\"\\bprogrammer\\b\", r\"\\bweb\\b\", r\"\\bui\\b\", r\"\\bux\\b\",\n",
    "    r\"\\bcomputer\\b\",\n",
    "]\n",
    "WEAK_KEYWORDS = [\n",
    "    r\"\\bite\\b\", r\"\\btech(?:nology)?\\b\", r\"\\bautomation\\b\", r\"\\brobot(?:ic|ics)\\b\",\n",
    "]\n",
    "STRONG_NO_BOUNDARY = [\n",
    "    \"software\",\"developer\",\"engineer\",\"devops\",\"ai\",\"ml\",\"cloud\",\n",
    "    \"cyber\",\"security\",\"data\",\"database\",\"network\",\"programmer\",\"web\",\"ui\",\"ux\",\"computer\",\n",
    "]\n",
    "STRONG_RE = [re.compile(p, re.I) for p in STRONG_KEYWORDS]\n",
    "WEAK_RE   = [re.compile(p, re.I) for p in WEAK_KEYWORDS]\n",
    "\n",
    "BASE_THRESHOLD = 2\n",
    "THRESHOLD_BY_SOC = {\"15\": 1, \"17\": 2, \"11\": 2, \"13\": 2}\n",
    "\n",
    "def keyword_hits(title_norm: str) -> Tuple[int, int, int]:\n",
    "    s = sum(1 for r in STRONG_RE if r.search(title_norm))\n",
    "    w = sum(1 for r in WEAK_RE   if r.search(title_norm))\n",
    "    l = 0 if s > 0 else sum(1 for sub in STRONG_NO_BOUNDARY if sub in title_norm)\n",
    "    return s, w, l\n",
    "\n",
    "def effective_threshold(major: Optional[str], base: int = BASE_THRESHOLD) -> int:\n",
    "    return THRESHOLD_BY_SOC.get(major or \"\", base)\n",
    "\n",
    "def is_tech(title: Optional[str], soc_code: Optional[str],\n",
    "            *, base_threshold: int = BASE_THRESHOLD,\n",
    "            return_reason: bool = False):\n",
    "    \"\"\"\n",
    "    Keep order:\n",
    "      1) hard excludes (normalized substring)\n",
    "      2) MUST_HAVE / INCLUDE (normalized substring)\n",
    "      3) SOC 15 default keep (unless normalized SOC15_EXCLUDE)\n",
    "      4) otherwise → SOC gate + keyword score ≥ per-SOC threshold\n",
    "    \"\"\"\n",
    "    t = _norm_text(title)\n",
    "    code = _norm_code(soc_code)\n",
    "    major = code.split(\"-\")[0] if code else None\n",
    "\n",
    "    # 1) hard excludes\n",
    "    if any(x in t for x in NORM_EXCLUDE):\n",
    "        out = (False, \"hard_exclude_match\")\n",
    "        return out if return_reason else out[0]\n",
    "\n",
    "    # 2) hard includes (must-haves + includes), robust to case/spacing\n",
    "    if any(x in t for x in NORM_INCLUDE):\n",
    "        out = (True, \"hard_include_or_must_have\")\n",
    "        return out if return_reason else out[0]\n",
    "\n",
    "    # 3) SOC 15 default keep (unless explicitly excluded)\n",
    "    if major in SOC_ALWAYS:\n",
    "        if any(x in t for x in NORM_SOC15_EXCLUDE):\n",
    "            out = (False, \"soc15_explicit_exclude\")\n",
    "            return out if return_reason else out[0]\n",
    "        out = (True, \"soc15_default_keep\")\n",
    "        return out if return_reason else out[0]\n",
    "\n",
    "    # 4) scoring + SOC gate\n",
    "    s, w, l = keyword_hits(t)\n",
    "    score = s + 0.5 * w + (1 if l >= 2 else 0)\n",
    "\n",
    "    if major in SOC_ENGINEER:\n",
    "        soc_pass = any(k in t for k in [\"computer\", \"electrical\", \"electronics\", \"robot\"])\n",
    "    elif major in SOC_OPTIONAL:\n",
    "        soc_pass = (s >= 1) or (score >= 2.5)\n",
    "    else:\n",
    "        soc_pass = False\n",
    "\n",
    "    thr = effective_threshold(major, base_threshold)\n",
    "    keep = soc_pass and (score >= thr)\n",
    "\n",
    "    reason = f\"soc_gate={soc_pass}, soc={major}, thr={thr}, score={score:.1f} (strong={s}, weak={w}, glued={l})\"\n",
    "    out = (keep, reason)\n",
    "    return out if return_reason else out[0]\n",
    "\n",
    "def apply_is_tech(df: pd.DataFrame,\n",
    "                  title_col: str = \"2023_national_employment_matrix_title\",\n",
    "                  code_col: str  = \"2023_national_employment_matrix_code\",\n",
    "                  base_threshold: int = BASE_THRESHOLD,\n",
    "                  with_reason: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Add _is_tech (and _tech_reason) using the classifier above.\"\"\"\n",
    "    titles = df[title_col].tolist()\n",
    "    codes  = df[code_col].tolist()\n",
    "    results = [\n",
    "        is_tech(t, c, base_threshold=base_threshold, return_reason=with_reason)\n",
    "        for t, c in zip(titles, codes)\n",
    "    ]\n",
    "    out = df.copy()\n",
    "    if with_reason:\n",
    "        flags, reasons = zip(*results) if results else ([], [])\n",
    "        out[\"_is_tech\"] = list(flags)\n",
    "        out[\"_tech_reason\"] = list(reasons)\n",
    "    else:\n",
    "        out[\"_is_tech\"] = results\n",
    "    return out\n",
    "\n",
    "# Kaggle-specific tech tagging\n",
    "def is_tech_kaggle(title: Optional[str],\n",
    "                   skills: Optional[str] = None,\n",
    "                   description: Optional[str] = None,\n",
    "                   *,\n",
    "                   base_threshold: int = 1,\n",
    "                   return_reason: bool = False):\n",
    "    \"\"\"Looser tech classifier for Kaggle: look in title + skills + description.\"\"\"\n",
    "    t = _norm_text(title)\n",
    "    s_text = _norm_text(skills) if skills else \"\"\n",
    "    d_text = _norm_text(description) if description else \"\"\n",
    "    combined = f\"{t} {s_text} {d_text}\".strip()\n",
    "\n",
    "    # hard excludes / includes reuse your global lists\n",
    "    if any(x in combined for x in NORM_EXCLUDE):\n",
    "        out = (False, \"hard_exclude_match\")\n",
    "        return out if return_reason else out[0]\n",
    "    if any(x in combined for x in NORM_INCLUDE):\n",
    "        out = (True, \"hard_include_or_must_have\")\n",
    "        return out if return_reason else out[0]\n",
    "\n",
    "    # keyword score (looser threshold)\n",
    "    s, w, l = keyword_hits(combined)\n",
    "    score = s + 0.5*w + (1 if l >= 2 else 0)\n",
    "    keep = score >= base_threshold\n",
    "    reason = f\"kaggle_score={score:.1f} (strong={s}, weak={w}, glued={l})\"\n",
    "    out = (keep, reason)\n",
    "    return out if return_reason else out[0]\n",
    "\n",
    "\n",
    "def apply_is_tech_kaggle(df: pd.DataFrame,\n",
    "                         title_col: str = \"job_title\",\n",
    "                         skills_col: Optional[str] = None,\n",
    "                         desc_col: Optional[str] = \"description\",\n",
    "                         *,\n",
    "                         base_threshold: int = 1,\n",
    "                         with_reason: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Apply Kaggle classifier over title + skills + description.\"\"\"\n",
    "    titles = df[title_col] if title_col in df.columns else pd.Series([\"\"]*len(df), index=df.index)\n",
    "    skills = df[skills_col] if (skills_col and skills_col in df.columns) else pd.Series([\"\"]*len(df), index=df.index)\n",
    "    descs  = df[desc_col]  if (desc_col  and desc_col  in df.columns)  else pd.Series([\"\"]*len(df), index=df.index)\n",
    "\n",
    "    results = [\n",
    "        is_tech_kaggle(t, s, d, base_threshold=base_threshold, return_reason=with_reason)\n",
    "        for t, s, d in zip(titles, skills, descs)\n",
    "    ]\n",
    "    out = df.copy()\n",
    "    if with_reason:\n",
    "        flags, reasons = zip(*results) if results else ([], [])\n",
    "        out[\"_is_tech\"] = list(flags)\n",
    "        out[\"_tech_reason\"] = list(reasons)\n",
    "    else:\n",
    "        out[\"_is_tech\"] = results\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1dc23812-ab7f-48af-9783-1e77fa2901c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# -------------------------------------------------------------------\n",
    "# 2) Generic cleaner (works for 1.2, 1.3, 1.4, 1.10; skip tech for 1.11)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def clean_bls_table(\n",
    "    xls_path: str,\n",
    "    sheet_name: str,\n",
    "    *,\n",
    "    numeric_cols: List[str] | None = None,\n",
    "    line_item_only: bool = True,\n",
    "    drop_footer_on: str = \"2023_national_employment_matrix_title\",\n",
    "    apply_tech_filter: bool = True,\n",
    "    title_col: str = \"2023_national_employment_matrix_title\",\n",
    "    code_col: str  = \"2023_national_employment_matrix_code\",\n",
    "    base_threshold: int = 2,\n",
    "    tag_only: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generic BLS sheet cleaner:\n",
    "      - load sheet, standardize columns\n",
    "      - replace lone dash/em-dash → NaN (warning-free)\n",
    "      - (opt) keep only 'Line item'\n",
    "      - (opt) drop trailing footers using `drop_footer_on`\n",
    "      - coerce numerics in `numeric_cols`\n",
    "      - (opt) tag tech via `apply_is_tech`\n",
    "      - returns tagged (all rows) if tag_only=True, else tech-only rows\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xls_path, sheet_name=sheet_name, skiprows=1)\n",
    "    df = clean_columns(df)\n",
    "    df = normalize_dash_missing(df)\n",
    "\n",
    "    if line_item_only and \"occupation_type\" in df.columns:\n",
    "        df = df[df[\"occupation_type\"].eq(\"Line item\")].copy()\n",
    "\n",
    "    if drop_footer_on in df.columns:\n",
    "        df = drop_footer_rows(df, drop_footer_on)\n",
    "\n",
    "    if numeric_cols:\n",
    "        df = coerce_numeric_if_exists(df, numeric_cols)\n",
    "\n",
    "    if apply_tech_filter:\n",
    "        df = apply_is_tech(\n",
    "            df,\n",
    "            title_col=title_col,\n",
    "            code_col=code_col,\n",
    "            base_threshold=base_threshold,\n",
    "            with_reason=True,\n",
    "        )\n",
    "        return df.reset_index(drop=True) if tag_only else df[df[\"_is_tech\"]].reset_index(drop=True)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7f9a62a8-d18b-45e2-bc7b-ddca5716bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 3) Batch runner + QC for BLS\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class SheetConfig:\n",
    "    sheet_name: str\n",
    "    file_stem: str                      # base filename to save (e.g., \"bls_table_1_2\")\n",
    "    numeric_cols: List[str]\n",
    "    line_item_only: bool = True\n",
    "    drop_footer_on: str = \"2023_national_employment_matrix_title\"\n",
    "    apply_tech_filter: bool = True\n",
    "    title_col: str = \"2023_national_employment_matrix_title\"\n",
    "    code_col: str  = \"2023_national_employment_matrix_code\"\n",
    "    must_have_check: bool = True        # run must-have QC (only if apply_tech_filter)\n",
    "    tag_only_save: bool = True       \n",
    "\n",
    "def qc_bls_generic(tagged: pd.DataFrame,\n",
    "                   tech: Optional[pd.DataFrame] = None,\n",
    "                   *,\n",
    "                   title_col: str = \"2023_national_employment_matrix_title\",\n",
    "                   code_col: str  = \"2023_national_employment_matrix_code\",\n",
    "                   must_have_titles: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"Reusable QC for any BLS sheet with title + code.\"\"\"\n",
    "    print(\"==== Shapes ====\")\n",
    "    print(\"Tagged full:\", tagged.shape)\n",
    "    if tech is not None:\n",
    "        print(\"Tech only  :\", tech.shape)\n",
    "\n",
    "    if title_col in tagged.columns and code_col in tagged.columns:\n",
    "        print(\"\\n==== Duplicates (tagged) ====\")\n",
    "        keys = [code_col, title_col]\n",
    "        print(tagged.duplicated(keys).sum())\n",
    "    if tech is not None and title_col in tech.columns and code_col in tech.columns:\n",
    "        print(\"\\n==== Duplicates (tech) ====\")\n",
    "        keys = [code_col, title_col]\n",
    "        print(tech.duplicated(keys).sum())\n",
    "\n",
    "    if tech is not None and code_col in tech.columns:\n",
    "        print(\"\\n==== SOC major distribution (tech) ====\")\n",
    "        soc = tech[code_col].astype(str).str.split(\"-\").str[0]\n",
    "        print(soc.value_counts())\n",
    "\n",
    "        print(\"\\n==== Missing SOC-15 (informational) ====\")\n",
    "        soc15_all = tagged[tagged[code_col].astype(str).str.startswith(\"15-\")][[title_col, code_col]]\n",
    "        soc15_tech = tech[tech[code_col].astype(str).str.startswith(\"15-\")][[title_col, code_col]]\n",
    "        missing = soc15_all.merge(soc15_tech, how=\"left\", on=[title_col, code_col], indicator=True)\\\n",
    "                           .query(\"_merge=='left_only'\")\n",
    "        print(missing.shape[0])\n",
    "        if not missing.empty:\n",
    "            print(missing.head(10))\n",
    "\n",
    "    if must_have_titles and tech is not None and title_col in tech.columns:\n",
    "        norm = (tech[title_col].astype(str).str.normalize(\"NFKC\").str.strip().str.lower())\n",
    "        present = set(norm.tolist())\n",
    "        must_norm = [m.lower().strip() for m in must_have_titles]\n",
    "        missing_must = [m for m in must_norm if m not in present]\n",
    "        hits = [m for m in must_norm if m in present]\n",
    "        print(\"\\n==== Must-have check ====\")\n",
    "        print(\"Present:\", [h.title() for h in hits])\n",
    "        print(\"Missing:\", [m.title() for m in missing_must])\n",
    "\n",
    "def batch_clean_bls(\n",
    "    xls_path: str,\n",
    "    out_dir: str,\n",
    "    sheets: List[SheetConfig],\n",
    "    *,\n",
    "    base_threshold: int = 2,\n",
    "    run_qc: bool = True,\n",
    ") -> Dict[str, Tuple[pd.DataFrame, Optional[pd.DataFrame]]]:\n",
    "    \"\"\"\n",
    "    Batch-clean multiple BLS sheets.\n",
    "    Returns {file_stem: (tagged_df, tech_df_or_None)}\n",
    "    Saves CSVs:\n",
    "      - <file_stem>_tagged.csv (if tag_only_save=True)\n",
    "      - <file_stem>.csv        (tech-only if apply_tech_filter=True; else cleaned sheet)\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    outputs: Dict[str, Tuple[pd.DataFrame, Optional[pd.DataFrame]]] = {}\n",
    "\n",
    "    for cfg in sheets:\n",
    "        print(f\"\\n=== Cleaning {cfg.sheet_name} → {cfg.file_stem} ===\")\n",
    "\n",
    "        # Tagged (keep-all)\n",
    "        tagged = clean_bls_table(\n",
    "            xls_path,\n",
    "            cfg.sheet_name,\n",
    "            numeric_cols=cfg.numeric_cols,\n",
    "            line_item_only=cfg.line_item_only,\n",
    "            drop_footer_on=cfg.drop_footer_on,\n",
    "            apply_tech_filter=cfg.apply_tech_filter,\n",
    "            title_col=cfg.title_col,\n",
    "            code_col=cfg.code_col,\n",
    "            base_threshold=base_threshold,\n",
    "            tag_only=True,\n",
    "        )\n",
    "        tech_df: Optional[pd.DataFrame] = None\n",
    "\n",
    "        if cfg.tag_only_save:\n",
    "            save_csv_file(tagged, out_dir, f\"{cfg.file_stem}_tagged.csv\")\n",
    "\n",
    "        if cfg.apply_tech_filter:\n",
    "            tech_df = clean_bls_table(\n",
    "                xls_path,\n",
    "                cfg.sheet_name,\n",
    "                numeric_cols=cfg.numeric_cols,\n",
    "                line_item_only=cfg.line_item_only,\n",
    "                drop_footer_on=cfg.drop_footer_on,\n",
    "                apply_tech_filter=True,\n",
    "                title_col=cfg.title_col,\n",
    "                code_col=cfg.code_col,\n",
    "                base_threshold=base_threshold,\n",
    "                tag_only=False,\n",
    "            )\n",
    "            save_csv_file(tech_df, out_dir, f\"{cfg.file_stem}.csv\")\n",
    "        else:\n",
    "            save_csv_file(tagged, out_dir, f\"{cfg.file_stem}.csv\")\n",
    "\n",
    "        if run_qc:\n",
    "            if cfg.apply_tech_filter:\n",
    "                qc_bls_generic(\n",
    "                    tagged=tagged,\n",
    "                    tech=tech_df,\n",
    "                    title_col=cfg.title_col,\n",
    "                    code_col=cfg.code_col,\n",
    "                    must_have_titles=MUST_HAVE_TITLES if cfg.must_have_check else None,\n",
    "                )\n",
    "            else:\n",
    "                qc_bls_generic(tagged=tagged, tech=None, title_col=cfg.title_col, code_col=cfg.code_col)\n",
    "\n",
    "        outputs[cfg.file_stem] = (tagged, tech_df)\n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4d3c2aa0-6c41-4bfe-887c-e3cadb6d06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# WEF builders + QC\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------\n",
    "# Canonical skill mapping\n",
    "# ---------------------------\n",
    "# Keep skill wording consistent across WEF tables so joins are stable.\n",
    "\n",
    "SKILL_CANON_MAP = {\n",
    "    \"ai and big data\": \"AI & Big Data\",\n",
    "    \"networks and cybersecurity\": \"Networks & Cybersecurity\",\n",
    "    \"technological literacy\": \"Technology Literacy\",\n",
    "    \"technology literacy\": \"Technology Literacy\",\n",
    "    \"programming\": \"Programming\",\n",
    "    \"analytical thinking\": \"Analytical Thinking\",\n",
    "    \"creative thinking\": \"Creative Thinking\",\n",
    "    \"curiosity and lifelong learning\": \"Curiosity & Lifelong Learning\",\n",
    "    \"leadership and social influence\": \"Leadership & Social Influence\",\n",
    "    \"systems thinking\": \"Systems Thinking\",\n",
    "    \"environmental stewardship\": \"Environmental Stewardship\",\n",
    "    \"design and user experience\": \"Design & UX\",\n",
    "}\n",
    "def canon_skill(x: str) -> str:\n",
    "    key = _norm_text(x)\n",
    "    return SKILL_CANON_MAP.get(key, x.strip())\n",
    "\n",
    "def build_wef_skill_growth() -> pd.DataFrame:\n",
    "    df = _load_csv(\"wef_core_skills_page_35.csv\")\n",
    "    if \"net_increase_pct\" not in df.columns:\n",
    "        df = df.rename(columns={\"net_increase_%\": \"net_increase_pct\"})\n",
    "    df[\"skill\"] = df[\"skill\"].map(canon_skill)\n",
    "    df = coerce_numeric_if_exists(df, [\"net_increase_pct\"])\n",
    "    _pct_bounds(df, [\"net_increase_pct\"], \"wef_skill_growth\")\n",
    "    return df.sort_values(\"net_increase_pct\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def build_wef_skill_industry() -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for fname, skill_label in [\n",
    "        (\"wef_ai_big_data_page_39.csv\", \"AI & Big Data\"),\n",
    "        (\"wef_tech_literacy_page_39.csv\", \"Technology Literacy\"),\n",
    "        (\"wef_networks_cyber_page_39.csv\", \"Networks & Cybersecurity\"),\n",
    "    ]:\n",
    "        df = _load_csv(fname)\n",
    "        df = df.rename(columns={\"percentage_pct\": \"pct_increasing\"})\n",
    "        df[\"skill\"] = skill_label\n",
    "        frames.append(df[[\"skill\", \"industry\", \"pct_increasing\"]])\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out[\"industry\"] = out[\"industry\"].map(lambda s: s.strip() if isinstance(s, str) else s)\n",
    "    out = coerce_numeric_if_exists(out, [\"pct_increasing\"])  # <-- fixed name\n",
    "    _pct_bounds(out, [\"pct_increasing\"], \"wef_skill_industry\")\n",
    "    return out.sort_values([\"skill\", \"pct_increasing\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "def build_wef_genai_substitution() -> pd.DataFrame:\n",
    "    df = _load_csv(\"wef_genai_substitution_page_b3_1_page_44.csv\")\n",
    "    df[\"skill_group\"] = df[\"skill_group\"].map(canon_skill)\n",
    "    df = coerce_numeric_if_exists(df, [\"very_low_capacity_pct\",\"low_capacity_pct\",\"moderate_capacity_pct\",\"high_capacity_pct\"])  # <-- fixed name\n",
    "    _pct_bounds(df, [\"very_low_capacity_pct\",\"low_capacity_pct\",\"moderate_capacity_pct\",\"high_capacity_pct\"], \"wef_genai_substitution\")\n",
    "    row_sum = df[[\"very_low_capacity_pct\",\"low_capacity_pct\",\"moderate_capacity_pct\",\"high_capacity_pct\"]].sum(axis=1)\n",
    "    off = (row_sum - 100).abs() > 5\n",
    "    if off.any():\n",
    "        print(\"[QC INFO] substitution rows not summing ~100% (tolerated due to chart eyeballing)\")\n",
    "        print(df.loc[off, [\"skill_group\",\"very_low_capacity_pct\",\"low_capacity_pct\",\"moderate_capacity_pct\",\"high_capacity_pct\"]].head())\n",
    "    return df.rename(columns={\"skill_group\":\"skill\"})\n",
    "\n",
    "def build_wef_training_completion() -> pd.DataFrame:\n",
    "    df = _load_csv(\"wef_training_completion_page_46.csv\")\n",
    "    df = df.rename(columns={\"training_completion_pct\":\"training_completion_2025_pct\"})\n",
    "    df[\"industry\"] = df[\"industry\"].map(lambda s: s.strip() if isinstance(s, str) else s)\n",
    "    df = coerce_numeric_if_exists(df, [\"training_completion_2025_pct\"])\n",
    "    _pct_bounds(df, [\"training_completion_2025_pct\"], \"wef_training_completion\")\n",
    "    return df.sort_values(\"training_completion_2025_pct\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def load_wef_job_lists() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    growing = _load_csv(\"wef_fastest_growing_jobs_page_19.csv\")\n",
    "    declining = _load_csv(\"wef_fastest_declining_jobs_page_19.csv\")\n",
    "    growing = growing.rename(columns={\"net_growth_%\":\"net_growth_pct\"})\n",
    "    declining = declining.rename(columns={\"net_job_destruction_(millions)\":\"net_job_destruction_millions\"})\n",
    "    growing[\"job_title_norm\"] = growing[\"job_title\"].map(_norm_text)\n",
    "    declining[\"job_title_norm\"] = declining[\"job_title\"].map(_norm_text)\n",
    "    return growing, declining\n",
    "\n",
    "def build_all_wef(save: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    skill_growth   = build_wef_skill_growth()\n",
    "    skill_industry = build_wef_skill_industry()\n",
    "    genai_subst    = build_wef_genai_substitution()\n",
    "    training       = build_wef_training_completion()\n",
    "    out = {\n",
    "        \"wef_skill_growth\": skill_growth,\n",
    "        \"wef_skill_industry\": skill_industry,\n",
    "        \"wef_genai_substitution\": genai_subst,\n",
    "        \"wef_training_completion\": training,\n",
    "    }\n",
    "    if save:\n",
    "        save_csv_file(skill_growth,   OUT_DIR, \"wef_skill_growth_tidy.csv\")    \n",
    "        save_csv_file(skill_industry, OUT_DIR, \"wef_skill_industry_tidy.csv\")\n",
    "        save_csv_file(genai_subst,    OUT_DIR, \"wef_genai_substitution_tidy.csv\")\n",
    "        save_csv_file(training,       OUT_DIR, \"wef_training_completion_tidy.csv\")\n",
    "    return out\n",
    "\n",
    "def qc_wef_tables(dfs: Dict[str, pd.DataFrame]):\n",
    "    sg = dfs[\"wef_skill_growth\"]\n",
    "    si = dfs[\"wef_skill_industry\"]\n",
    "    gs = dfs[\"wef_genai_substitution\"]\n",
    "    tr = dfs[\"wef_training_completion\"]\n",
    "\n",
    "    print(\"\\n=== Skill growth (top 6) ===\")\n",
    "    print(sg.head(6))\n",
    "\n",
    "    print(\"\\n=== Skill × Industry (AI & Big Data) — top 6 ===\")\n",
    "    print(si[si[\"skill\"].eq(\"AI & Big Data\")].head(6))\n",
    "\n",
    "    print(\"\\n=== GenAI substitution (sample) ===\")\n",
    "    print(gs.head(6))\n",
    "\n",
    "    print(\"\\n=== Training completion by industry (top 6) ===\")\n",
    "    print(tr.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1d62f073-1e2e-4063-8795-42267242c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Kaggle builders + QC\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "WEF_CANON_SKILLS = [\n",
    "    \"AI & Big Data\",\n",
    "    \"Networks & Cybersecurity\",\n",
    "    \"Technology Literacy\",\n",
    "    \"Programming\",\n",
    "    \"Design & UX\",\n",
    "]\n",
    "\n",
    "CANON_RULES: Dict[str, List[re.Pattern]] = {\n",
    "    \"AI & Big Data\": [\n",
    "        re.compile(r\"\\b(ai|artificial intelligence|machine learning|ml|deep learning|pytorch|tensorflow|nlp|llm|prompt)\\b\", re.I),\n",
    "        re.compile(r\"\\b(data(science| scientist| analyst| engineering)?|etl|warehouse|warehousing|spark|pyspark|hadoop|snowflake|databricks|airflow|kafka|hive)\\b\", re.I),\n",
    "        re.compile(r\"\\b(statistics?|probability|pandas|numpy|matplotlib|seaborn)\\b\", re.I),\n",
    "    ],\n",
    "    \"Programming\": [\n",
    "        re.compile(r\"\\b(program(ming|mer)?|software|developer|engineer|devops|sre|site reliability)\\b\", re.I),\n",
    "        re.compile(r\"\\b(python|java|javascript|typescript|go|golang|rust|swift|kotlin|scala|c\\+\\+|c#|sql|bash|shell|linux|powershell|regex)\\b\", re.I),\n",
    "        re.compile(r\"\\b(node|react|angular|vue|django|flask|spring|fastapi|.net|dotnet|pytest|junit)\\b\", re.I),\n",
    "        re.compile(r\"\\b(ci/?cd|jenkins|github actions|gitlab ci)\\b\", re.I),\n",
    "    ],\n",
    "    \"Networks & Cybersecurity\": [\n",
    "        re.compile(r\"\\b(cyber|information security|infosec|security analyst|siem|soc|iam|zero trust|pen(etration)? test|cissp|nist|iso 27001)\\b\", re.I),\n",
    "        re.compile(r\"\\b(network|firewall|vpn|ids|ips)\\b\", re.I),\n",
    "    ],\n",
    "    \"Design & UX\": [\n",
    "        re.compile(r\"\\b(ui|ux|user experience|product design|figma|sketch|wireframe|usability|user research|interaction design)\\b\", re.I),\n",
    "    ],\n",
    "    \"Technology Literacy\": [\n",
    "        # keep broad/cloud/infra here\n",
    "        re.compile(r\"\\b(aws|azure|gcp|cloud|kubernetes|docker|terraform|ansible|vmware)\\b\", re.I),\n",
    "        re.compile(r\"\\b(service now|servicenow|itil|sre practices)\\b\", re.I),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "JUNK_TOKENS = {\n",
    "    \"skills\", \"and\", \"etc\", \"na\", \"n/a\",\n",
    "    \"bachelor\", \"master\", \"masters\", \"phd\", \"degree\",\n",
    "    \"related to education:\", \"related to experience:\", \"experience-related skills:\",\n",
    "    \"microsoft office\", \"documentation\", \"presentation\",\n",
    "    \"communication\", \"written communication\", \"verbal communication\",\n",
    "    \"strong written communication\", \"interpersonal skills\",\n",
    "    \"project management\", \"portfolio analysis\",\n",
    "    \"****\"\n",
    "}\n",
    "# very short tokens like 'r', 'c' are often noise unless explicitly mapped\n",
    "MIN_LEN = 3\n",
    "\n",
    "def split_skill_field(val: Optional[str]) -> List[str]:\n",
    "    if not isinstance(val, str) or not val.strip():\n",
    "        return []\n",
    "    v = re.sub(r\"[A-Za-z\\- ]*Skills:\\s*\", \"\", val, flags=re.I)\n",
    "    parts = re.split(r\"[;,|/]\", v)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        tok = _norm_text(p)\n",
    "        if not tok or tok in JUNK_TOKENS:\n",
    "            continue\n",
    "        if len(tok) < MIN_LEN and tok not in {\"r\", \"c\"}:\n",
    "            continue\n",
    "        out.append(tok)\n",
    "    return out\n",
    "\n",
    "def map_raw_skill_to_canon(raw_skill: str) -> List[str]:\n",
    "    s = _norm_text(raw_skill)\n",
    "    if not s:\n",
    "        return []\n",
    "    matches = []\n",
    "    for canon, patterns in CANON_RULES.items():\n",
    "        if any(p.search(s) for p in patterns):\n",
    "            matches.append(canon)\n",
    "    return matches or [\"Technology Literacy\"]  # gentle fallback\n",
    "\n",
    "# ---------- loaders ----------\n",
    "def load_jobs_dataset_processed() -> pd.DataFrame:\n",
    "    path = os.path.join(RAW_DIR, \"jobs_dataset_processed.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    return _norm_colnames(df)  # id, query, job_title, description, it_skills, soft_skills, education, experience,...\n",
    "\n",
    "def load_ai_job_dataset() -> pd.DataFrame:\n",
    "    path = os.path.join(RAW_DIR, \"ai_job_dataset.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    return _norm_colnames(df)  # job_id, job_title, salary_usd, required_skills, posting_date, ...\n",
    "\n",
    "# ---------- cleaners ----------\n",
    "def clean_jobs_dataset_processed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"job_title_norm\"] = out[\"job_title\"].map(normalize_title)\n",
    "\n",
    "    out[\"description_norm\"] = out[\"description\"].astype(str).fillna(\"\").map(_norm_text)\n",
    "    out[\"description_hash\"] = out[\"description_norm\"].map(md5_hash_text)\n",
    "\n",
    "    skill_cols = [c for c in [\"it_skills\",\"soft_skills\",\"education\",\"experience\"] if c in out.columns]\n",
    "    out[\"skills_blob\"] = out[skill_cols].astype(str).agg(\" | \".join, axis=1) if skill_cols else \"\"\n",
    "\n",
    "    out = apply_is_tech_kaggle(out,\n",
    "                               title_col=\"job_title\",\n",
    "                               skills_col=\"skills_blob\",\n",
    "                               desc_col=\"description\",\n",
    "                               base_threshold=1,\n",
    "                               with_reason=True)\n",
    "\n",
    "    out = out.sort_values(\"id\", na_position=\"last\")\n",
    "    out = out.drop_duplicates(subset=[\"job_title_norm\",\"description_hash\"], keep=\"first\")\n",
    "    return out\n",
    "\n",
    "def clean_ai_job_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"job_title_norm\"] = out[\"job_title\"].map(normalize_title)\n",
    "\n",
    "    if \"posting_date\" in out.columns:\n",
    "        out[\"posting_date\"] = safe_to_datetime(out[\"posting_date\"])\n",
    "        out[\"post_month\"] = out[\"posting_date\"].dt.to_period(\"M\").astype(str)\n",
    "    for c in [\"salary_usd\",\"benefits_score\",\"years_experience\",\"job_description_length\",\"remote_ratio\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    skill_cols = [c for c in [\"required_skills\",\"education_required\"] if c in out.columns]\n",
    "    out[\"skills_blob\"] = out[skill_cols].astype(str).agg(\" | \".join, axis=1) if skill_cols else \"\"\n",
    "\n",
    "    # pick whichever description column exists\n",
    "    desc_col_name = \"job_description\" if \"job_description\" in out.columns else (\"description\" if \"description\" in out.columns else None)\n",
    "\n",
    "    out = apply_is_tech_kaggle(out,\n",
    "                               title_col=\"job_title\",\n",
    "                               skills_col=\"skills_blob\",\n",
    "                               desc_col=desc_col_name,\n",
    "                               base_threshold=1,\n",
    "                               with_reason=True)\n",
    "\n",
    "    if {\"company_name\",\"post_month\"}.issubset(out.columns):\n",
    "        out = out.sort_values([\"company_name\",\"post_month\"]).drop_duplicates(\n",
    "            subset=[\"company_name\",\"job_title_norm\",\"post_month\"], keep=\"first\"\n",
    "        )\n",
    "    else:\n",
    "        key = (out[\"job_title_norm\"].fillna(\"\") + \"|\" +\n",
    "               out.get(\"industry\",\"\").astype(str).fillna(\"\") + \"|\" +\n",
    "               out.get(\"company_location\",\"\").astype(str).fillna(\"\"))\n",
    "        out[\"surrogate_hash\"] = key.map(md5_hash_text)\n",
    "        out = out.drop_duplicates(subset=[\"surrogate_hash\"], keep=\"first\")\n",
    "    return out\n",
    "\n",
    "# ---------- explode skills to tidy ----------\n",
    "def explode_skills_from_jobs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    skill_cols = [c for c in [\"it_skills\",\"soft_skills\",\"education\",\"experience\"] if c in out.columns]\n",
    "    rows = []\n",
    "    for _, r in out.iterrows():\n",
    "        pid = r.get(\"id\"); title = r.get(\"job_title_norm\")\n",
    "        for sc in skill_cols:\n",
    "            for tok in split_skill_field(r.get(sc)):\n",
    "                rows.append((\"jobs_dataset_processed\", pid, title, tok))\n",
    "    tidy = pd.DataFrame(rows, columns=[\"source\",\"posting_id\",\"job_title_norm\",\"raw_skill\"])\n",
    "    records = []\n",
    "    for _, rr in tidy.iterrows():\n",
    "        for canon in map_raw_skill_to_canon(rr[\"raw_skill\"]):\n",
    "            records.append((rr[\"source\"], rr[\"posting_id\"], rr[\"job_title_norm\"], rr[\"raw_skill\"], canon))\n",
    "    return pd.DataFrame(records, columns=[\"source\",\"posting_id\",\"job_title_norm\",\"raw_skill\",\"canon_skill\"])\n",
    "\n",
    "def explode_skills_from_ai(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    rows = []\n",
    "    for _, r in out.iterrows():\n",
    "        pid = r.get(\"job_id\"); title = r.get(\"job_title_norm\")\n",
    "        for tok in split_skill_field(r.get(\"required_skills\")):\n",
    "            rows.append((\"ai_job_dataset\", pid, title, tok))\n",
    "        for tok in split_skill_field(r.get(\"education_required\")):\n",
    "            rows.append((\"ai_job_dataset\", pid, title, tok))\n",
    "    tidy = pd.DataFrame(rows, columns=[\"source\",\"posting_id\",\"job_title_norm\",\"raw_skill\"])\n",
    "    records = []\n",
    "    for _, rr in tidy.iterrows():\n",
    "        for canon in map_raw_skill_to_canon(rr[\"raw_skill\"]):\n",
    "            records.append((rr[\"source\"], rr[\"posting_id\"], rr[\"job_title_norm\"], rr[\"raw_skill\"], canon))\n",
    "    return pd.DataFrame(records, columns=[\"source\",\"posting_id\",\"job_title_norm\",\"raw_skill\",\"canon_skill\"])\n",
    "\n",
    "# ---------- aggregations ----------\n",
    "def build_posting_metrics_ai(df_ai: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"post_month\" not in df_ai.columns:\n",
    "        return pd.DataFrame(columns=[\"job_title_norm\",\"post_month\",\"postings\",\"median_salary_usd\",\"remote_ratio_avg\"])\n",
    "    grp = df_ai.groupby([\"job_title_norm\",\"post_month\"], dropna=False)\n",
    "    met = grp.agg(\n",
    "        postings=(\"job_title_norm\",\"count\"),\n",
    "        median_salary_usd=(\"salary_usd\",\"median\"),\n",
    "        remote_ratio_avg=(\"remote_ratio\",\"mean\"),\n",
    "    ).reset_index()\n",
    "    return met.sort_values([\"job_title_norm\",\"post_month\"])\n",
    "\n",
    "def build_skill_counts(tidy_skills: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    raw_counts = tidy_skills.groupby(\"raw_skill\", dropna=False).size().reset_index(name=\"count\")\\\n",
    "                            .sort_values(\"count\", ascending=False)\n",
    "    canon_counts = tidy_skills.groupby(\"canon_skill\", dropna=False).size().reset_index(name=\"count\")\\\n",
    "                              .sort_values(\"count\", ascending=False)\n",
    "    return raw_counts, canon_counts\n",
    "\n",
    "def build_title_summary(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    grp = df_all.groupby(\"job_title_norm\", dropna=False).size().reset_index(name=\"postings\")\n",
    "    return grp.sort_values(\"postings\", ascending=False)\n",
    "\n",
    "# ---------- orchestrator + QC ----------\n",
    "def run_kaggle_clean(save: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    jobs = load_jobs_dataset_processed()\n",
    "    ai   = load_ai_job_dataset()\n",
    "\n",
    "    jobs_c = clean_jobs_dataset_processed(jobs)\n",
    "    ai_c   = clean_ai_job_dataset(ai)\n",
    "\n",
    "    jobs_tech = jobs_c[jobs_c[\"_is_tech\"]].copy()\n",
    "    ai_tech   = ai_c[ai_c[\"_is_tech\"]].copy()\n",
    "\n",
    "    skills_jobs = explode_skills_from_jobs(jobs_tech)\n",
    "    skills_ai   = explode_skills_from_ai(ai_tech)\n",
    "    skills_all  = pd.concat([skills_jobs, skills_ai], ignore_index=True)\n",
    "\n",
    "    ai_metrics                  = build_posting_metrics_ai(ai_tech)\n",
    "    raw_counts, canon_counts    = build_skill_counts(skills_all)\n",
    "    title_summary               = build_title_summary(pd.concat(\n",
    "                                        [jobs_tech[[\"job_title_norm\"]],\n",
    "                                         ai_tech[[\"job_title_norm\"]]],\n",
    "                                        ignore_index=True))\n",
    "\n",
    "    if save:\n",
    "        save_csv_file(jobs_tech,     OUT_DIR, \"kaggle_jobs_tech.csv\")\n",
    "        save_csv_file(ai_tech,       OUT_DIR, \"kaggle_ai_tech.csv\")\n",
    "        save_csv_file(skills_all,    OUT_DIR, \"kaggle_skills_tidy.csv\")\n",
    "        save_csv_file(ai_metrics,    OUT_DIR, \"kaggle_ai_metrics_monthly.csv\")\n",
    "        save_csv_file(raw_counts,    OUT_DIR, \"kaggle_skill_counts_raw.csv\")\n",
    "        save_csv_file(canon_counts,  OUT_DIR, \"kaggle_skill_counts_canon.csv\")\n",
    "        save_csv_file(title_summary, OUT_DIR, \"kaggle_title_summary.csv\")\n",
    "\n",
    "    return {\n",
    "        \"jobs_tech\": jobs_tech,\n",
    "        \"ai_tech\": ai_tech,\n",
    "        \"skills_all\": skills_all,\n",
    "        \"ai_metrics\": ai_metrics,\n",
    "        \"skill_counts_raw\": raw_counts,\n",
    "        \"skill_counts_canon\": canon_counts,\n",
    "        \"title_summary\": title_summary,\n",
    "    }\n",
    "\n",
    "def qc_kaggle(dfs: Dict[str, pd.DataFrame]) -> None:\n",
    "    print(\"\\n=== Kaggle shapes ===\")\n",
    "    for k, v in dfs.items():\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            print(k, v.shape)\n",
    "\n",
    "    if not dfs[\"ai_tech\"].empty:\n",
    "        print(\"\\n=== Sample AI titles (salary + month) ===\")\n",
    "        print(dfs[\"ai_tech\"][[\"job_title_norm\",\"salary_usd\",\"post_month\"]].head(8))\n",
    "\n",
    "    print(\"\\n=== Top canonical skills ===\")\n",
    "    print(dfs[\"skill_counts_canon\"].head(10))\n",
    "\n",
    "    if not dfs[\"ai_metrics\"].empty:\n",
    "        print(\"\\n=== Monthly postings (sample) ===\")\n",
    "        print(dfs[\"ai_metrics\"].head(10))\n",
    "\n",
    "    print(\"\\n=== Top titles by postings (both datasets) ===\")\n",
    "    print(dfs[\"title_summary\"].head(10))\n",
    "\n",
    "def check_salary_coverage(df, title_col=\"job_title_norm\", salary_col=\"median_salary_usd\", min_pct=0.5):\n",
    "    \"\"\"\n",
    "    Checks if each title has at least `min_pct` non-null salary coverage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe containing job title and salary columns.\n",
    "    title_col : str\n",
    "        Column with normalized job titles.\n",
    "    salary_col : str\n",
    "        Column with salary values.\n",
    "    min_pct : float\n",
    "        Minimum required proportion of months with salary data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Titles with salary coverage below `min_pct`.\n",
    "    \"\"\"\n",
    "    salary_cov = (\n",
    "        df.groupby(title_col)[salary_col]\n",
    "          .apply(lambda x: x.notna().mean())\n",
    "    )\n",
    "\n",
    "    low_cov = salary_cov[salary_cov < min_pct]\n",
    "\n",
    "    if low_cov.empty:\n",
    "        print(f\"All titles have >= {min_pct*100:.0f}% salary coverage.\")\n",
    "    else:\n",
    "        print(f\"Titles with < {min_pct*100:.0f}% salary coverage:\")\n",
    "        print(low_cov)\n",
    "\n",
    "    return low_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0b37a963-263c-4f5e-b0cc-234af3321bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Table 1.2 → bls_table_1_2 ===\n",
      "Saved: data/processed_data/bls_table_1_2_tagged.csv | shape=(832, 18)\n",
      "Saved: data/processed_data/bls_table_1_2.csv | shape=(22, 18)\n",
      "==== Shapes ====\n",
      "Tagged full: (832, 18)\n",
      "Tech only  : (22, 18)\n",
      "\n",
      "==== Duplicates (tagged) ====\n",
      "0\n",
      "\n",
      "==== Duplicates (tech) ====\n",
      "0\n",
      "\n",
      "==== SOC major distribution (tech) ====\n",
      "2023_national_employment_matrix_code\n",
      "15    21\n",
      "11     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==== Missing SOC-15 (informational) ====\n",
      "0\n",
      "\n",
      "==== Must-have check ====\n",
      "Present: ['Software Developers', 'Data Scientists', 'Information Security Analysts', 'Computer Systems Analysts', 'Database Administrators', 'Database Architects', 'Computer Network Architects', 'Computer User Support Specialists', 'Web Developers', 'Web And Digital Interface Designers', 'Software Quality Assurance Analysts And Testers', 'Computer Programmers', 'Computer And Information Systems Managers']\n",
      "Missing: []\n",
      "\n",
      "=== Cleaning Table 1.3 → bls_table_1_3 ===\n",
      "Saved: data/processed_data/bls_table_1_3_tagged.csv | shape=(32, 9)\n",
      "Saved: data/processed_data/bls_table_1_3.csv | shape=(7, 9)\n",
      "==== Shapes ====\n",
      "Tagged full: (32, 9)\n",
      "Tech only  : (7, 9)\n",
      "\n",
      "==== Duplicates (tagged) ====\n",
      "0\n",
      "\n",
      "==== Duplicates (tech) ====\n",
      "0\n",
      "\n",
      "==== SOC major distribution (tech) ====\n",
      "2023_national_employment_matrix_code\n",
      "15    6\n",
      "11    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==== Missing SOC-15 (informational) ====\n",
      "0\n",
      "\n",
      "==== Must-have check ====\n",
      "Present: ['Software Developers', 'Data Scientists', 'Information Security Analysts', 'Computer And Information Systems Managers']\n",
      "Missing: ['Computer Systems Analysts', 'Database Administrators', 'Database Architects', 'Computer Network Architects', 'Computer User Support Specialists', 'Web Developers', 'Web And Digital Interface Designers', 'Software Quality Assurance Analysts And Testers', 'Computer Programmers']\n",
      "\n",
      "=== Cleaning Table 1.4 → bls_table_1_4 ===\n",
      "Saved: data/processed_data/bls_table_1_4_tagged.csv | shape=(32, 9)\n",
      "Saved: data/processed_data/bls_table_1_4.csv | shape=(3, 9)\n",
      "==== Shapes ====\n",
      "Tagged full: (32, 9)\n",
      "Tech only  : (3, 9)\n",
      "\n",
      "==== Duplicates (tagged) ====\n",
      "0\n",
      "\n",
      "==== Duplicates (tech) ====\n",
      "0\n",
      "\n",
      "==== SOC major distribution (tech) ====\n",
      "2023_national_employment_matrix_code\n",
      "15    2\n",
      "11    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==== Missing SOC-15 (informational) ====\n",
      "0\n",
      "\n",
      "==== Must-have check ====\n",
      "Present: ['Software Developers', 'Data Scientists', 'Computer And Information Systems Managers']\n",
      "Missing: ['Information Security Analysts', 'Computer Systems Analysts', 'Database Administrators', 'Database Architects', 'Computer Network Architects', 'Computer User Support Specialists', 'Web Developers', 'Web And Digital Interface Designers', 'Software Quality Assurance Analysts And Testers', 'Computer Programmers']\n",
      "\n",
      "=== Cleaning Table 1.10 → bls_table_1_10 ===\n",
      "Saved: data/processed_data/bls_table_1_10_tagged.csv | shape=(1113, 16)\n",
      "Saved: data/processed_data/bls_table_1_10.csv | shape=(29, 16)\n",
      "==== Shapes ====\n",
      "Tagged full: (1113, 16)\n",
      "Tech only  : (29, 16)\n",
      "\n",
      "==== Duplicates (tagged) ====\n",
      "0\n",
      "\n",
      "==== Duplicates (tech) ====\n",
      "0\n",
      "\n",
      "==== SOC major distribution (tech) ====\n",
      "2023_national_employment_matrix_code\n",
      "15    28\n",
      "11     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==== Missing SOC-15 (informational) ====\n",
      "0\n",
      "\n",
      "==== Must-have check ====\n",
      "Present: ['Software Developers', 'Data Scientists', 'Information Security Analysts', 'Computer Systems Analysts', 'Database Administrators', 'Database Architects', 'Computer Network Architects', 'Computer User Support Specialists', 'Web Developers', 'Web And Digital Interface Designers', 'Software Quality Assurance Analysts And Testers', 'Computer Programmers', 'Computer And Information Systems Managers']\n",
      "Missing: []\n",
      "\n",
      "=== Cleaning Table 1.11 → bls_table_1_11 ===\n",
      "Saved: data/processed_data/bls_table_1_11_tagged.csv | shape=(4, 6)\n",
      "Saved: data/processed_data/bls_table_1_11.csv | shape=(4, 6)\n",
      "==== Shapes ====\n",
      "Tagged full: (4, 6)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Run BLS\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "NUMS_12 = [\n",
    "    \"employment_2023\",\"employment_2033\",\n",
    "    \"employment_distribution_percent_2023\",\"employment_distribution_percent_2033\",\n",
    "    \"employment_change_numeric_2023-33\",\"employment_change_percent_2023-33\",\n",
    "    \"percent_self_employed_2023\",\"occupational_openings_2023-33_annual_average\",\n",
    "    \"median_annual_wage_dollars_2024\",\n",
    "]\n",
    "NUMS_13_14 = [\n",
    "    \"employment_2023\",\"employment_2033\",\n",
    "    \"employment_change_numeric_2023-33\",\"employment_change_percent_2023-33\",\n",
    "    \"median_annual_wage_dollars_2024\",\n",
    "]\n",
    "NUMS_110 = [\n",
    "    \"employment_2023\",\"employment_2033\",\n",
    "    \"employment_change_numeric_2023-33\",\"employment_change_percent_2023-33\",\n",
    "    \"labor_force_exit_rate_2023-33_annual_average\",\n",
    "    \"occupational_transfer_rate_2023-33_annual_average\",\n",
    "    \"total_occupational_separations_rate_2023-33_annual_average\",\n",
    "    \"labor_force_exits_2023-33_annual_average\",\n",
    "    \"occupational_transfers_2023-33_annual_average\",\n",
    "    \"total_occupational_separations_2023-33_annual_average\",\n",
    "    \"occupational_openings_2023-33_annual_average\",\n",
    "]\n",
    "NUMS_111 = [\n",
    "    \"employment_2023\",\"employment_2033\",\n",
    "    \"employment_change_numeric_2023-33\",\"employment_change_percent_2023-33\",\n",
    "    \"median_annual_wage_dollars_2024\",\n",
    "]\n",
    "\n",
    "xls_path = \"data/raw/bls_occupation.xlsx\"\n",
    "out_dir  = \"data/processed_data\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "sheets_cfg = [\n",
    "        SheetConfig(\"Table 1.2\",  \"bls_table_1_2\",  NUMS_12,    line_item_only=True,  must_have_check=True),\n",
    "        SheetConfig(\"Table 1.3\",  \"bls_table_1_3\",  NUMS_13_14, line_item_only=False, must_have_check=True),\n",
    "        SheetConfig(\"Table 1.4\",  \"bls_table_1_4\",  NUMS_13_14, line_item_only=False, must_have_check=True),\n",
    "        SheetConfig(\"Table 1.10\", \"bls_table_1_10\", NUMS_110,   line_item_only=False, must_have_check=True),\n",
    "        # Table 1.11 — different schema; keep all rows, no tech filter\n",
    "        SheetConfig(\"Table 1.11\", \"bls_table_1_11\", NUMS_111,   line_item_only=False,\n",
    "                    apply_tech_filter=False, drop_footer_on=\"occupation_category\", must_have_check=False),\n",
    "]\n",
    "\n",
    "outputs = batch_clean_bls(\n",
    "        xls_path=xls_path,\n",
    "        out_dir=out_dir,\n",
    "        sheets=sheets_cfg,\n",
    "        base_threshold=2,\n",
    "        run_qc=True,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c7725c0f-1054-41ad-9342-8518c545c9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QC INFO] substitution rows not summing ~100% (tolerated due to chart eyeballing)\n",
      "                     skill_group  very_low_capacity_pct  low_capacity_pct  \\\n",
      "0                  AI & Big Data                      0                 8   \n",
      "1                    Programming                      0                35   \n",
      "7  Curiosity & Lifelong Learning                      4                45   \n",
      "\n",
      "   moderate_capacity_pct  high_capacity_pct  \n",
      "0                     78                 20  \n",
      "1                     45                 37  \n",
      "7                     41                  3  \n",
      "Saved: data/processed_data/wef_skill_growth_tidy.csv | shape=(12, 2)\n",
      "Saved: data/processed_data/wef_skill_industry_tidy.csv | shape=(30, 3)\n",
      "Saved: data/processed_data/wef_genai_substitution_tidy.csv | shape=(9, 5)\n",
      "Saved: data/processed_data/wef_training_completion_tidy.csv | shape=(15, 2)\n",
      "\n",
      "=== Skill growth (top 6) ===\n",
      "                                 skill  net_increase_pct\n",
      "0                        AI & Big Data                87\n",
      "1             Networks & Cybersecurity                70\n",
      "2                  Technology Literacy                68\n",
      "3                    Creative Thinking                66\n",
      "4  Resilience, flexibility and agility                66\n",
      "5        Curiosity & Lifelong Learning                61\n",
      "\n",
      "=== Skill × Industry (AI & Big Data) — top 6 ===\n",
      "           skill                                industry  pct_increasing\n",
      "0  AI & Big Data                Automotive and Aerospace             100\n",
      "1  AI & Big Data                      Telecommunications             100\n",
      "2  AI & Big Data                   Professional Services              98\n",
      "3  AI & Big Data     Information and Technology Services              97\n",
      "4  AI & Big Data       Insurance and Pensions Management              97\n",
      "5  AI & Big Data  Financial Services and Capital Markets              95\n",
      "\n",
      "=== GenAI substitution (sample) ===\n",
      "                      skill  very_low_capacity_pct  low_capacity_pct  \\\n",
      "0             AI & Big Data                      0                 8   \n",
      "1               Programming                      0                35   \n",
      "2       Technology Literacy                      1                58   \n",
      "3  Networks & Cybersecurity                      2                76   \n",
      "4               Design & UX                      0                63   \n",
      "5       Analytical Thinking                      3                83   \n",
      "\n",
      "   moderate_capacity_pct  high_capacity_pct  \n",
      "0                     78                 20  \n",
      "1                     45                 37  \n",
      "2                     38                  3  \n",
      "3                     23                  2  \n",
      "4                     36                  0  \n",
      "5                     10                  2  \n",
      "\n",
      "=== Training completion by industry (top 6) ===\n",
      "                                 industry  training_completion_2025_pct\n",
      "0         Supply Chain and Transportation                            58\n",
      "1                      Telecommunications                            52\n",
      "2       Insurance and Pensions Management                            52\n",
      "3  Financial Services and Capital Markets                            50\n",
      "4         Medical and Healthcare Services                            50\n",
      "5     Information and Technology Services                            47\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Run WEF\n",
    "# -------------------------------------------------------------------\n",
    "wef = build_all_wef(save=True)\n",
    "qc_wef_tables(wef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "51b992aa-f542-4704-8d42-93abacb51d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/processed_data/kaggle_jobs_tech.csv | shape=(2393, 15)\n",
      "Saved: data/processed_data/kaggle_ai_tech.csv | shape=(4666, 24)\n",
      "Saved: data/processed_data/kaggle_skills_tidy.csv | shape=(74260, 5)\n",
      "Saved: data/processed_data/kaggle_ai_metrics_monthly.csv | shape=(320, 5)\n",
      "Saved: data/processed_data/kaggle_skill_counts_raw.csv | shape=(22246, 2)\n",
      "Saved: data/processed_data/kaggle_skill_counts_canon.csv | shape=(5, 2)\n",
      "Saved: data/processed_data/kaggle_title_summary.csv | shape=(1944, 2)\n",
      "\n",
      "=== Kaggle shapes ===\n",
      "jobs_tech (2393, 15)\n",
      "ai_tech (4666, 24)\n",
      "skills_all (74260, 5)\n",
      "ai_metrics (320, 5)\n",
      "skill_counts_raw (22246, 2)\n",
      "skill_counts_canon (5, 2)\n",
      "title_summary (1944, 2)\n",
      "\n",
      "=== Sample AI titles (salary + month) ===\n",
      "            job_title_norm  salary_usd post_month\n",
      "435   ai software engineer      100678    2024-01\n",
      "1132        data scientist       89495    2024-01\n",
      "1419    ai product manager       68838    2024-01\n",
      "1986         ai specialist       51821    2024-01\n",
      "2215     robotics engineer       51621    2024-01\n",
      "2553            head of ai       48677    2024-01\n",
      "3955          ai architect       71826    2024-01\n",
      "4169         data engineer       54304    2024-01\n",
      "\n",
      "=== Top canonical skills ===\n",
      "                canon_skill  count\n",
      "4       Technology Literacy  52925\n",
      "0             AI & Big Data  11468\n",
      "3               Programming   9130\n",
      "2  Networks & Cybersecurity    632\n",
      "1               Design & UX    105\n",
      "\n",
      "=== Monthly postings (sample) ===\n",
      "  job_title_norm post_month  postings  median_salary_usd  remote_ratio_avg\n",
      "0   ai architect    2024-01        16            80317.5         68.750000\n",
      "1   ai architect    2024-02        15            90175.0         36.666667\n",
      "2   ai architect    2024-03        16            85604.0         65.625000\n",
      "3   ai architect    2024-04        15           117841.0         40.000000\n",
      "4   ai architect    2024-05        15           144798.0         43.333333\n",
      "5   ai architect    2024-06        16            84159.5         43.750000\n",
      "6   ai architect    2024-07        15           106780.0         36.666667\n",
      "7   ai architect    2024-08        16            79898.0         40.625000\n",
      "8   ai architect    2024-09        14           106482.5         53.571429\n",
      "9   ai architect    2024-10        16           104791.5         59.375000\n",
      "\n",
      "=== Top titles by postings (both datasets) ===\n",
      "                   job_title_norm  postings\n",
      "417                  data analyst       286\n",
      "532                data scientist       284\n",
      "479                 data engineer       281\n",
      "1065    machine learning engineer       256\n",
      "45           ai software engineer       247\n",
      "1294     principal data scientist       247\n",
      "1222                 nlp engineer       245\n",
      "156   autonomous systems engineer       245\n",
      "853                    head of ai       244\n",
      "362      computer vision engineer       244\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Run Kaggle\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "kaggle_out = run_kaggle_clean(save=True)\n",
    "qc_kaggle(kaggle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e4d71fcc-69c3-4487-8f62-3cd9efbe99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All titles have >= 50% salary coverage.\n"
     ]
    }
   ],
   "source": [
    "low_cov_titles = check_salary_coverage(ai_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7eac79-fdfa-4ee4-bdce-1ebf0b1f0def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
